% Source of the template:
% https://fit.cvut.cz/cs/studium/programy-a-obory/doktorske/dsp-informatika/pro-stavajici-studenty/sablony
% Šablona prezentace

% Original presentation:
% https://github.com/filipbartek/neural-precedence-recommender/blob/master/presentation.tex

%\documentclass[a4paper,notes]{beamer}
\documentclass[a4paper]{beamer}

\setbeamertemplate{footline}[page number]
%
% Josef Hlavac & Tomas Zahradnicky (21.10.2010)
%
%\usepackage{pgfpages}
%\pgfpagesuselayout{4 on 1}[a4paper,border shrink=5mm,landscape]
%\pgfpagesuselayout{resize}[a4paper,border shrink=5mm,landscape]
\usepackage[utf8]{inputenc}
\usepackage{color}
\usetheme{Frankfurt}
\useinnertheme{circles}
\usepackage{subfigure}
\usepackage[english]{babel}

% glossaries should be loaded after hyperref.
\input{tex/glossary}

\input{tex/preamble}

% https://tex.stackexchange.com/a/33612/202639
\def\FuncSigmoid(#1){1.0/(1.0 + exp(-(#1)))}

\usefonttheme{professionalfonts}
%
\def\uv#1{\char92\relax #1\char34\relax}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\Email{filip.bartek@cvut.cz}
\newcommand\DissertationTitle{Recommending Symbol Precedences\\by Machine Learning}
\newcommand\University{Czech Technical University in Prague}
\newcommand\FacultyAndUniversityAbbr{CTU}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author[F. Bártek]{Filip Bártek}
\title{\DissertationTitle}
\titlegraphic{\includegraphics[width=1.5cm]{pic/LogoCVUT.pdf}}
\institute[\FacultyAndUniversityAbbr]{\University}
\date{\DTMdate{2022-03-23}}
%
\begin{document}
\begin{frame}
%\begin{center}
%\includegraphics[width=1.5cm]{pic/LogoCVUT.pdf}
%\end{center}
\titlepage
\end{frame}

\section{Example problem}

\input{tex/example}

\section{Precedence recommendation}

\begin{frame}
\frametitle{\Gls{fol} with equality}
Examples of clauses:
\begin{itemize}
\item $x + 0 = x$
\item $x + s(y) = s(x + y)$
\item $0 + s(s(0)) \neq s(s(0))$
\item $\lnot \mli{Nat}(x) \lor x + s(y) = s(x + y)$
\item $\lnot R(x, y) \lor \lnot R(y, z) \lor R(x, z)$
\item $\lnot R(x, y) \lor \lnot R(y, x) \lor x = y$
\item $\lnot R(x, y) \lor x = y \lor S(x, y)$
\item $\lnot P \lor \lnot Q \lor R \lor S$
\end{itemize}
\note{We use infix notation for functions and predicates where appropriate.}
\end{frame}

\begin{frame}
\frametitle{Precedence recommendation}
\begin{itemize}
\item Input: a \gls{fol} problem in \gls{cnf}
\item Output: a symbol precedence
\begin{itemize}
\item Try to minimize the runtime.
\end{itemize}
\item Target algorithm: an \acrfull{atp}
\begin{itemize}
\item A fixed configuration
\item Ordered resolution
\item Superposition calculus
\end{itemize}
\item Challenge: unaligned signatures across problems
\note{In our example problem, we could replace $+$ with $f$ to obtain an equivalent problem.}
\item State of the art: simple heuristics (for example \texttt{invfreq})
\end{itemize}
\end{frame}

\section{Our recommender}

\begin{frame}
\frametitle{Our precedence recommender}
\begin{itemize}
\item \Acrlong{ml} from \acrshort{atp} runs with random precedences
\item Generalizes across problems
\begin{itemize}
\item Symbol semantics
\item Signature length
\end{itemize}
\item Approach:
\begin{itemize}
\item \Acrshort{ml} core: \Gls{gcn}
\item Nested proxy tasks:
\begin{itemize}
\item Precedence pair classification
\item Symbol cost regression
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Graph representation of a \acrshort{cnf} problem}
% Mention: relational graph, node types, edge types

Input problem: $z + 0 = z \land x + s(y) = s(x + y)$

\centering
\input{tex/cnf-graph-diagram}
\note{Each clause binds all of its variables. Thus, variable $x$ is not shared across the two clauses.}
\end{frame}

\begin{frame}
\frametitle{Architecture}
\centering
\input{tex/gcn-recommender-diagram}
\end{frame}

\section{Evaluation}

\begin{frame}
\frametitle{Evaluation}

\fontsize{10pt}{12}\selectfont

\centering
\input{tex/evaluation-table}

\end{frame}

\section{Conclusion}
\begin{frame}
\frametitle{Summary}
\begin{itemize}
\item \Acrfull{fol} problem corresponds to a graph
\item \Acrfull{gcn} predicts symbol costs
\item Sorting symbols by costs yields a precedence
\item Training data: precedence pairs ``$\Better{\PrecBetter}{\PrecWorse}{P}$'' \\
(``Precedence $\PrecBetter$ is better than precedence $\PrecWorse$ in problem $P$.'')
\item Final recommender outperforms the ``frequency'' heuristic by \SI{4.8}{\percent} on \acrshort{tptp}
\end{itemize}
\end{frame}

\begin{frame}
\begin{center}
\vspace*{1cm}
{\bf Thank you for your attention!}\\
\vspace*{2cm}
{\bf\Large Filip Bártek}\\
{\tt \Email}
\vspace*{1cm}
\end{center}

%\vfill
% https://tex.stackexchange.com/a/142950/202639
%{\tiny {\bf Acknowledgement:} This work was supported by
%the Czech Science Foundation project no.~20-06390Y (JUNIOR grant),
%the project RICAIP no.~857306 under the EU-H2020 programme,
%and
%the Grant Agency of the Czech Technical University in Prague, grant
%no.~SGS20/215/OHK3/3T/37.\par}
\end{frame}
%
\appendix

\section{Review}
\input{tex/questions-answers}

\section{Technical details}

\begin{frame}
\frametitle{Notation overview}
\begin{center}
\begin{tabular}{ll}
$\pi(i)$ & index of the $i$-th symbol in precedence $\pi$ \\
$c$ & vector of symbol costs (output of the \acrshort{gcn}) \\
$c_i$ & cost of the $i$-th symbol \\
$C(\pi)$ & cost of symbol precedence $\pi$ \\
$\loss(P, \PrecBetter, \PrecWorse)$ & loss for training example $\Better{\PrecBetter}{\PrecWorse}{P}$ \\
\end{tabular}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Prediction of symbol precedence $\pi$}
\begin{algorithmic} % enter the algorithmic environment
\REQUIRE Problem $P$
\ENSURE Symbol precedence $\pi$
\STATE $c \gets \mathrm{\acrshort{gcn}}(P)$
\COMMENT {Forward pass through the \acrshort{gcn} to obtain vector of symbol costs $c$}
\STATE $\pi \gets \argsort^- (c_1, \ldots, c_n)$
\COMMENT {Sort symbols by $c$ in non-increasing order to obtain precedence $\pi$}
\RETURN $\pi$
\end{algorithmic}
\end{frame}

\begin{frame}
\frametitle{\Acrfull{gcn}}

Initial embedding of node $d$:
$$
h_d^{(0)} = \textrm{(feature vector)} \oplus \textrm{(trainable vector)}
$$

Feature vector:
\begin{itemize}
\item Clause: role (axiom, assumption, negated conjecture)
\item Symbol: introduced in preprocessing, in conjecture
\end{itemize}

Propagation rule for layer $l$:
$$
h_d^{(l+1)} =
\sum_{r \in \mathcal{R}} \sigma \Parentheses*{\sum_{s \in \mathcal{N}_d^r} \frac{1}{\sqrt{\card{\mathcal{N}_s^r}} \sqrt{\card{\mathcal{N}_d^r}}} (W_r^{(l)} h_s^{(l)} + b_r^{(l)})}
$$

\end{frame}

\section{Training}

\begin{frame}
\frametitle{Precedence cost}
Reminder: $c_i$ is the cost of the $i$-th symbol.

\begin{block}{Cost of symbol precedence $\pi$ over signature of length $n$}
\begin{equation*}
C(\pi) = \frac{2}{n(n+1)} \sum_{i=1}^n i \cdot c_{\pi(i)}
\end{equation*}
\end{block}

\begin{lemma}[Precedence cost minimization]
The precedence cost $C$ is minimized by any precedence that sorts the symbols by their costs in non-increasing order:
$$
\argmin_{\rho \in \mathrm{Perm}(n)} C(\rho) = \argsort^- (c_1, \ldots, c_n)
$$
\end{lemma}

\end{frame}

\begin{frame}
\frametitle{Proof sketch: Precedence cost minimization}

$C(\pi) = \frac{2}{n(n+1)} \sum_{i=1}^n i \cdot c_{\pi(i)}$

\begin{lemma}[Precedence cost minimization]
$$
\argmin_{\rho \in \mathrm{Perm}(n)} C(\rho) = \argsort^- (c_1, \ldots, c_n)
$$
\end{lemma}

\newcommand{\StairsSorted}{
\begin{tikzpicture}[baseline=5pt]
\begin{axis}[ybar interval=1, axis lines=none, height=60pt, width=60pt]
\addplot[black] coordinates {(1,5) (2,4) (3,3) (4,2) (5,1) (6,0)};
\end{axis}
\end{tikzpicture}
}

\newcommand{\StairsUnsorted}{
\begin{tikzpicture}[baseline=5pt]
\begin{axis}[ybar interval=1, axis lines=none, height=60pt, width=60pt]
\addplot[black] coordinates {(1,5) (2,3) (3,4) (4,2) (5,1) (6,0)};
\end{axis}
\end{tikzpicture}
}

\begin{example}
$$
C\Parentheses*{\StairsSorted} < C\Parentheses*{\StairsUnsorted}
$$

Proof:
\begin{align*}
C\Parentheses*{\StairsUnsorted} - C\Parentheses*{\StairsSorted}
&\propto ((2 \cdot 3 + 3 \cdot 4) - (2 \cdot 4 + 3 \cdot 3)) \\
&= 18 - 17 = 1 > 0
\end{align*}
\end{example}

\end{frame}

\begin{frame}
\frametitle{Loss function}

\begin{exampleblock}{Training example $\Better{\PrecBetter}{\PrecWorse}{P}$}
Precedence $\PrecBetter$ is better than precedence $\PrecWorse$ for problem $P$.
\end{exampleblock}

Reminder: $C(\pi)$ is the predicted cost of precedence $\pi$.

\begin{block}{Loss on training example $\Better{\PrecBetter}{\PrecWorse}{P}$}
\begin{equation*}
\loss(P, \PrecBetter, \PrecWorse) = - \log \sigmoid (C(\PrecWorse) - C(\PrecBetter))
\end{equation*}

\centering
% https://pgf-tikz.github.io/pgf/pgfmanual.pdf : 94. Mathematical Expressions
\begin{tikzpicture}
\begin{axis}[
    domain=-12:12,
    xmin=-12,
    xmax=12,
    ymin=-1,
    ymax=11,
    samples=100,
    xlabel={$C(\PrecWorse) - C(\PrecBetter)$},
    ylabel={$\loss(P, \PrecBetter, \PrecWorse)$},
    scale only axis,
    width=0.5\textwidth,
    height=0.25\textwidth
]
%\addplot[gray] {0};
%\addplot[gray] {-x};
\addplot[] {-ln(\FuncSigmoid(x))};
\end{axis}
\end{tikzpicture}
\end{block}

\end{frame}

\begin{frame}
\frametitle{Concise representation of training examples}
\begin{block}{Cost of symbol precedence $\pi$ over signature of length $n$}
\begin{equation*}
C(\pi)
= \frac{2}{n(n+1)} \sum_{i=1}^n i \cdot c_{\pi(i)}
= \frac{2}{n(n+1)} \sum_{i=1}^n c_i \cdot \inv{\pi}(i)
\end{equation*}
\end{block}

\begin{block}{Loss of training example $\Better{\PrecBetter}{\PrecWorse}{P}$}
\begin{align*}
\loss(P, \PrecBetter, \PrecWorse)
&= - \log \sigmoid (C(\PrecWorse) - C(\PrecBetter)) \\
&= - \log \sigmoid \frac{2}{n(n+1)} \sum_{i=1}^n i (c_{\PrecWorse(i)} - c_{\PrecBetter(i)}) \\
&= - \log \sigmoid \frac{2}{n(n+1)} \sum_{i=1}^n c_i (\inv{\PrecWorse}(i) - \inv{\PrecBetter}(i))
\end{align*}
\end{block}

Concise representation of $\Better{\PrecBetter}{\PrecWorse}{P}$:
$\inv{\PrecWorse} - \inv{\PrecBetter}$
\end{frame}

\section{Impact of precedence on rewriting}

\begin{frame}
\frametitle{Precedence determines orientability of identities}
$$
f(x, a, y) \approx f(y, b, x)
$$

\begin{itemize}
\item If $a > f$ and $a > b$,
then $f(x, a, y) >_{lpo} f(y, b, x)$.
\item If $f > a$ and $f > b$,
then $f(x, a, y)$ and $f(y, b, x)$ are $>_{lpo}$-incomparable.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Precedence determines termination of completion}
Set of identities:
\begin{align*}
x + 0 &\approx x \\
x + s(y) &\approx s(x + y)
\end{align*}

\begin{itemize}
\item LPO($+ > s$):
Completion orients from left to right and terminates.
\item LPO($s > +$):
% See TRaAT, Example 7.1.3
Completion diverges:
\begin{align*}
x + 0 &\to x\\
x + s(0) &\to s(x)\\
x + s(s(0)) &\to s(s(x))\\
&\vdots\\
x + s^n(0) &\to s^n(x)\\
&\vdots
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Precedence may inflate the search space}
\begin{enumerate}
\item $>_{+s}$ = LPO($* > + > s$)
\item $>_{s+}$ = LPO($s > + > *$)
\end{enumerate}
\begin{align*}
x + 0 &\to x \\
x + s(y) &\approx s(x + y) \tag{$\to_{+s}$, $\leftarrow_{s+}$}\\
x * 0 &\to 0\\
x * s(y) &\to x + (x * y)
\end{align*}
\begin{enumerate}
\item With $>_{+s}$, the initial set is complete.
\item With $>_{s+}$, completion yields a large number of new rules:
\begin{align*}
x + s^n(0) &\to s^n(x) \tag{$\forall n \in \nat$}\\
x + (x * (x' + y')) &\approx x * (x' + s(y')) \tag{unorientable}
\end{align*}
\end{enumerate}
\end{frame}

\end{document}
